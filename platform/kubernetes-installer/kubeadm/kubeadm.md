# Kubeadm 部署 Kubernetes 集群

使用 kubeadm 默认会启动一个安全的 kubernetes 集群，但依然不建议在生产环境使用，原因有许多，比如：etcd 存在单点故障问题。

## 组件

| 组件名         | 含义 |
| -------------- | ---- |
| kubeadm        |      |
| kubelet        |      |
| kubectl        |      |
| kubernetes-cni |      |

## 应用

| 应用名     | 部署形式                      |     |
| ---------- | ----------------------------- | --- |
| kube-dns   | Service Replicaset Deployment |     |
| kube-proxy | Daemonset                     |     |

## 安装 Docker

Kubeadm 目前已验证的最大 Docker 版本是 `1.12.6`，安装 Dcoker 的详细步骤：[安装攻略](../../docker/docker-install.md)。

```bash
$ # 确保 docker 已在运行
$ docker info

$ # 验证 Cgroup driver
$ docker info | grep Cgroup
Cgroup Driver: cgroupfs
```

## 安装

`kubeadm` 软件包依赖 `kubectl`、`kubelet`、`kubernetes-cni`、`socat` 软件包；`kubelet` 软件包依赖 `kubernetes-cnt`、`socat` 软件包。

* **安装 kubeadm 等工具**

```bash
$ # 添加源
$ cat > /etc/yum.repos.d/kubernetes.repo <<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

$ # 缓存
$ yum makecache fast

$ # 关闭防火墙
$ systemctl stop firewalld && systemctl disable firewalld

$ # 临时禁用 SELinux
$ setenforce 0

$ # 安装最新版本
$ yum install -y kubelet kubeadm

$ # 安装指定版本（推荐。其依赖依然是最新版本，所以应该让更多的一）
$ yum install -y kubectl-1.6.2 kubelet-1.6.2 kubeadm-1.6.2 kubernetes-cni-0.5.1

$ # 设置 kuberlet 开机自启动
$ systemctl enable kubelet.service
```

默认安装好后，还会在 `/opt/cni/bin` 下安装一些网络工具。

* 修改 Cgroup driver**

在 CentOS 上初始化集群或者加入集群时，还可能出现如下错误：

```bash
$ journalctl -xef 或 tail -f /var/log/message 或 journalctl -f -u kubelet
$ # error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"

$ # 分析原因
$ # docker 的 cgroup driver 和 kubelet 的 cgroup driver 不一致，我们统一使用 cgroupfs

$ # 修改 kebelet 配置
$ sed -i "s|--cgroup-driver=systemd|--cgroup-driver=cgroupfs|g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
$ systemctl daemon-reload

$ # 检查 kubelete 配置
$ cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf | grep "cgroup-driver"
```

* **修改内核参数**

```bash
$ cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
$
$ # 立即生效
$ sysctl -p /etc/sysctl.d/k8s.conf
$
$ # 检验
$ sysctl net.bridge.bridge-nf-call-iptables
$ sysctl net.bridge.bridge-nf-call-ip6tables
```

* **获取镜像**

如果缺少相应的 `gcr.io/google_containers/*` 镜像，执行 `kubeadm init`、`kubeadm join` 等操作会被卡住，具体需要哪些版本的镜像可以在 `/etc/kubernetes/` 目录下查看 manifest。

```bash
$ # master 节点
$ ./k8s-master-image.sh v1.6.2

$ # node 节点
$ ./k8s-node-image.sh v1.6.2
```

## 部署

如果 `kubeadm init` 失败或者 `kubeadm join` 失败可以通过 `kubeadm reset` 来清除所有的修改，包括运行的容器以及 `/etc/kubernetes` 下的相关文件。如果节点已经加入了集群，可以通过 `kubectl delete node <node-name>` 命令将其踢出集群，再 `kubeadm reset`。

```bash
# 将 Node 节点踢出集群
$ kubectl delete node <node-name>

# 在 Node 节点清除 kubernetes
$ kubeadm reset

# 移除 docker
$ ops/docker/uninstall-docker-engine.sh

# 删除的更彻底一些
$ rm -rf /etc/kubernetes
$ rm -rf /etc/cni
$ rm -rf /opt/cni

# 移除虚拟网络接口
$ ip link delete cni0
$ ip link delete flannel.1
```

如果是 Master 节点：

```bash
$ kubectl delete all --all
$ kubectl delete namespace --all
$ kubectl delete pv --all
$ kubectl delete sc --all
$ kubeadm reset

# 删除的更彻底一些
$ rm -rf /etc/kubernetes
$ rm -rf /etc/cni
$ rm -rf /opt/cni

# 卸载
$ yum remove kubeadm kubectl kubelet -y
```

* **master**

```bash
$ # 检查本机私有 IP 是否正确且唯一，如果有误请修改 /etc/hosts 文件
$ hostname -i

$ # kubernete 使用和 kubeadm 相同的版本（kubeadm version）
$ kubeadm init --apiserver-advertise-address=$(hostname -i)

$ # 部署指定版本（推荐）
$ # --pod-network-cidr 默认为 10.244.0.0/16，如果要修改请与之后添加的网络插件的配置保持一致
$ # --service-cidr 默认为 10.96.0.0/16
$ kubeadm init --kubernetes-version=v1.6.2 --apiserver-advertise-address=$(hostname -i) --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/16

$ # 客户端（kubectl）需要有配置和证书，才能使用操作、管理 Kubernetes 集群

$ # 配置 kubectl （方法一）
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

$ # 配置 kubectl （方法二）
$ sudo cp /etc/kubernetes/admin.conf $HOME/
$ sudo chown $(id -u):$(id -g) $HOME/admin.conf
$ export KUBECONFIG=$HOME/admin.conf
$ echo "export KUBECONFIG=$HOME/admin.conf" >> ~/.bash_profile
$ source ~/.bash_profile

$ # 检查 kubectl 是否配置成功
$ kubectl get nodes

$ # 初始化 master 后如果忘了 token，可以通过下面的命令查看
$ kubeadm token list
TOKEN                     TTL         EXPIRES   USAGES                   DESCRIPTION
a1ed36.7bba409461c70635   <forever>   <never>   authentication,signing   The default bootstrap token generated by 'kubeadm init'.

$ # 检查服务是否都在运行
$ netstat -tpln | grep -Eo ".*(etcd|kube-schedule|kube-controll|kube-apiserve|kubelet|kube-proxy).*"

$ # 查看控件状态（componentstatuses）
$ kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}
```

默认情况下，kube-scheduler 是不会调度 Pod 到 master 节点上的，如果有需要可以通过下面的方式修改：

```bash
$ # 在某个 Master 上移除 taint
$ kubectl taint nodes k8s-master-1 node-role.kubernetes.io/master-

$ # 所有节点移除 taint
$ kubectl taint nodes --all node-role.kubernetes.io/master-
```

* **网络**

部署完集集群后并还需要手动部署集群网络插件，否则部分容器和 `kube-dns` 等插件将无法启动。

```bash
$ # 如果没有部署网络插件，节点处于 NotReady 状态
$ kubectl get nodes
NAME                 STATUS     AGE       VERSION
centos-compute-100   NotReady   37m       v1.6.2

$ # 部署前先看看哪些容器没有成功启动
$ kubectl get pods --all-namespaces
NAMESPACE     NAME                                         READY     STATUS    RESTARTS
kube-system   etcd-centos-compute-100                      1/1       Running   0
kube-system   kube-apiserver-centos-compute-100            1/1       Running   0
kube-system   kube-controller-manager-centos-compute-100   1/1       Running   0
kube-system   kube-dns-3913472980-vc7b6                    0/3       Pending   0
kube-system   kube-proxy-1s07h                             1/1       Running   0
kube-system   kube-scheduler-centos-compute-100            1/1       Running   0
```

部署 calico 网络插件：

```bash
$ kubectl apply -f ./manifests/calico.yaml
```

部署 flannel 网络插件：

```bash
$ kubectl apply -f ./manifests/kube-flannel-rbac.yaml
$ kubectl apply -f ./manifests/kube-flannel.yaml
```

部署 weave 网络插件：

```bash
$ kubectl apply -f ./manifests/weave.yaml
```

网络插件部署完成后，检查再检查一下服务是否都已经正常启动：

```bash
$ # 确保所有的 Pod 都已经处于 Running 状态
$ kubectl get pods --all-namespaces -o wide

$ # 确保所有的 node 都已经处于 Ready 状态
$ kubectl get nodes
```

如果网络不用了或者需要更换网络插件，还需要先手动删除它：

```bash
$ # 以 flannel 为例

$ # 先删除服务
$ kubectl delete -f ./manifests/kube-flannel-rbac.yaml
$ kubectl delete -f ./manifests/kube-flannel.yaml

$ # 在所有节点上删除相应的虚拟网卡
$ ifconfig cni0 down && ip link delete cni0
$ ifconfig flannel.1 down && ip link delete flannel.1
$ rm -rf /var/lib/cni/
$ rm -rf /etc/cni/net.d/
```

* **kube-dns**

部署完（不一定要等到成功）网络插件后，会自启动 kube-dns。kube-dns 这个 pod  一共包含三个容器：kubedns、kubemasq 和 sidecar，[详情](https://kubernetes.feisky.xyz/components/kube-dns-internal.html)。

```bash
$ # 查看需要用到哪些相关的镜像及版本
$ kubectl get deployment kube-dns -n kube-system -o yaml | grep "image:.*"
gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1

$ # 虽然网络已经启动，但 kube-dns 依然可能存在问题，很大可能是网络插件没有真正启动成功
$ kubectl describe pod kube-dns-3913472980-vc7b6 -n kube-system
Warning BackOff   Back-off restarting failed container
Warning FailedSync  Error syncing pod, skipping: [failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-3913472980-vc7b6_kube-system(6152eeb3-92c6-11e7-a3f9-408d5cfaed4a)"

$ # 检查 kubedns 容器
$ kubectl logs -f --since=1h -p kube-dns-3913472980-vc7b6 -c kubedns -n kube-system
I0906 10:03:23.208849       1 dns.go:174] DNS server not ready, retry in 500 milliseconds
I0906 10:03:23.708868       1 dns.go:174] DNS server not ready, retry in 500 milliseconds

$ # 检查 dnsmasq 容器
$ kubectl logs -f --since=1h -p kube-dns-3913472980-vc7b6 -c dnsmasq -n kube-system

$ # 检查 sidecar 容器
$ kubectl logs -f --since=1h -p kube-dns-3913472980-vc7b6 -c sidecar -n kube-system
```

测试 DNS：

```bash
$ # 创建一个 deployment
$ kubectl run dns-test --image=nginx:alpine
$
$ kubectl get pods
NAME                       READY     STATUS    RESTARTS   AGE
dns-test-549056164-gq0d6   1/1       Running   0          4m
$
$ # 检查是否可以正常解析
$ kubectl exec -it dns-test-549056164-gq0d6 nslookup kubernetes.default
Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
$
$ # 删除 deployment
$ kubectl delete deploy dns-test
```

* **node**

如果 master 部署好且 kube-dns、网络插件运行正常，可以加入新的节点。

```bash
$ # kubeadm join --token=<token> <ip-of-master>:<port-of-kube-apiserver>
$ kubeadm join --token=a1ed36.7bba409461c70635 10.0.13.3:6443

$ # 配置 kubectl

$ # 检验节点是否加入了集群。由于没有部署容器网络，node 处于 NotReady 状态
$ kubectl get nodes

$ # 检查服务是否都在运行
$ netstat -tpln | grep -Eo ".*(kubelet|kube-proxy).*"
```

* **Dashboard**

```bash
$ # Service 类型：ClusterIP
$ kubectl apply -f ./manifests/kubernetes-dashboard.yaml

$ # Service 类型：NodePort
$ kubectl apply -f ./manifests/kubernetes-dashboard-nodeport.yaml
```

* **Heapster**

节点上的 kubelet 内置的 cAdvisor 只提供了单机的资源监控，可以通过 `http://<node-ip>:4194` 查看监控情况。Heapster 为集群增加了监控和性能分析功能，支持将监控信息持久化到 influxDB、Elastic Search、Kafka 等[存储后端]()。

```bash
$ kubectl apply -f ./manifests/grafana.yaml
$ kubectl apply -f ./manifests/influxdb.yaml
$ kubectl apply -f ./manifests/heapster-rbac.yaml
$ kubectl apply -f ./manifests/heapster.yaml
```

* **代理**

可以通过 `http://<serviceip-of-heapster>/metrics` 查看监控信息。

```bash
$ # 在 master 节点上查看集群服务
$ kubectl cluster-info
Kubernetes master is running at https://192.168.10.100:6443
Heapster is running at https://192.168.10.100:6443/api/v1/proxy/namespaces/kube-system/services/heapster
KubeDNS is running at https://192.168.10.100:6443/api/v1/proxy/namespaces/kube-system/services/kube-dns
monitoring-grafana is running at https://192.168.10.100:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
monitoring-influxdb is running at https://192.168.10.100:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb

$ # 在 master 节点上启动代理
$ kubectl proxy --address='0.0.0.0' --port=8080 --accept-hosts='^*$'
```

## 参考

* [使用kubeadm安装Kubernetes 1.6](http://blog.frognew.com/2017/04/kubeadm-install-kubernetes-1.6.html#安装kubeadm和kubelet)
* [kubeadm 安装 Kubernetes-1.6.1 集群](http://www.iyunv.com/thread-383771-1-1.html)
* [Installing kubeadm](https://kubernetes.io/docs/setup/independent/install-kubeadm/)
* [Using kubeadm to Create a Cluster](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/)
* [Installing Addons](https://kubernetes.io/docs/concepts/cluster-administration/addons/)
* [Run Heapster in a Kubernetes cluster with an InfluxDB backend and a Grafana UI](https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md)
* [[kubeadm 搭建 Kubernetes 集群](https://www.kubernetes.org.cn/878.html)](https://www.kubernetes.org.cn/878.html)
* [当 Kubernetes 遇到阿里云 之 快速部署1.6.7版本](https://yq.aliyun.com/articles/73922?commentId=9510)
* [kubeadm 工作机制分析](http://blog.csdn.net/waltonwang/article/details/70162993)